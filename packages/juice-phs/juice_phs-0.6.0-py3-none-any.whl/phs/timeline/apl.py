import json
import os

# Function to create directories
def create_directories(base_path, directories):
    for directory in directories:
        full_path = os.path.join(base_path, directory)
        os.makedirs(full_path, exist_ok=True)
        print(f"Created directory: {full_path}")


# Function to modify and save a new JSON file for each unit
def create_json_files(base_path, directories, json_template, operational_period, reference_event, event_counter):
    # Load the provided JSON template from the string
    json_data = json.loads(json_template)

    for unit in directories:

        if unit == 'PEP':
            # Modify the "unit" field and save a new JSON file for each unit
            json_data["activities"][0]["unit"] = "PEL"
            json_data["activities"][0]["id"] = "PEL-01"
            json_data["activities"][0]["counter"] = f"{event_counter}"
            json_data["activities"][0]["event"] = f"{reference_event}"

            # Define the new filename and save the file
            new_filename = f"APL_{unit}_PEL_{operational_period}_S00P00.json"
            new_file_path = os.path.join(base_path, unit, new_filename)

            with open(new_file_path, 'w') as outfile:
                json.dump(json_data, outfile, indent=4)

            print(f"Created JSON file: {new_file_path}")

            # Modify the "unit" field and save a new JSON file for each unit
            json_data["activities"][0]["unit"] = "PEH"
            json_data["activities"][0]["id"] = "PEH-01"
            json_data["activities"][0]["counter"] = f"{event_counter}"
            json_data["activities"][0]["event"] = f"{reference_event}"

            # Define the new filename and save the file
            new_filename = f"APL_{unit}_PEH_{operational_period}_S00P00.json"
            new_file_path = os.path.join(base_path, unit, new_filename)

            with open(new_file_path, 'w') as outfile:
                json.dump(json_data, outfile, indent=4)

            print(f"Created JSON file: {new_file_path}")

        else:

            # Modify the "unit" field and save a new JSON file for each unit
            json_data["activities"][0]["unit"] = unit
            json_data["activities"][0]["id"] = f"{unit}-01"
            json_data["activities"][0]["counter"] = f"{event_counter}"
            json_data["activities"][0]["event"] = f"{reference_event}"

            # Define the new filename and save the file
            new_filename = f"APL_{unit}_{unit}_{operational_period}_S00P00.json"
            new_file_path = os.path.join(base_path, unit, new_filename)

            with open(new_file_path, 'w') as outfile:
                json.dump(json_data, outfile, indent=4)

            print(f"Created JSON file: {new_file_path}")


def generate_initial_files(base_path, operational_period="PCW3", reference_event="COEV", event_counter='3'):
    # Define the directories based on the last part of each path
    directories = [
        "3GM", "GAL", "JAN", "JMC", "MAG", "MAJ", "NAV", "PEP", "RIM", "RPW", "SWI", "SYS", "UVS"
    ]

    # Paths and operational period
    # Define the JSON template as a string
    json_template = """
    {
      "activities": [
        {
          "id": "Identification of the activity, e.g.: HAA-01",
          "unit": "HAA",
          "stack": "Identification for a number of activities that will be scheduled WRT the same event or will be in a POR, e.g.: HAA_1",
          "event": "COEV",
          "counter": 3,
          "duration": "Duration of the activity in +DDD.HH:MM:SS.MMM format, e.g: +000.01:30:00.000",
          "relative": "Relative time of the activity with respect to the event (and counter) in +DDD.HH:MM:SS.MMM format, e.g: -001.01:00:01.000",
          "description": "Free text with high-level description of the activity.",
          "data_resource": "String with data volume (DV) or data rate (DR) with the value and its associated units; DV allowed units are: bits, kbits, mbits, gbits; DR allowed units are: bps, kbps, mbps. E.g.: 35.5 mbits",
          "scheduling_rules": "Free text indicating scheduling rules for the activity.",
          "tc_budget": "Integer that provides the estimated number of TCs generated by the activity. e.g. 15 (without quotes)"
        }
      ]
    }
    """

    # Create directories
    create_directories(base_path, directories)

    # Create JSON files
    create_json_files(base_path, directories, json_template, operational_period, reference_event, event_counter)


def concatenate_files(directory, output_file, pattern='SXXPYY'):
    """
    Concatenate APL JSON files matching a pattern from a directory.

    Parameters
    ----------
    directory : str
        The directory to search for JSON files.
    output_file : str
        The file to write the concatenated JSON data.
    pattern : str, optional
        Pattern to match in file names (default is 'SXXPYY').

    Returns
    -------
    None
        This function does not return a value, it writes data to a file.
    """
    concatenated_activities = []

    # Walk through all subdirectories and find JSON files matching the pattern
    for root, dirs, files in os.walk(directory):
        if root != directory:  # Exclude top-level directory
            for file in files:
                if pattern in file and file.endswith(".json"):
                    file_path = os.path.join(root, file)
                    with open(file_path, 'r') as f:
                        try:
                            data = json.load(f)
                        except ValueError as e:
                            print(f'Invalid APL {file_path}: %s' % e)
                            return None  # or: raise
                        if "activities" in data:
                            concatenated_activities.extend(data["activities"])

    # Write the concatenated activities to the output file
    with open(output_file, 'w+') as f:
        json.dump({"activities": concatenated_activities}, f, indent=4)

    return


def check_duplicate_ids(apl_file):
    """
    Check for duplicate IDs in an APL JSON file.

    Parameters
    ----------
    apl_file : str
        The path to the JSON file to check for duplicate IDs.

    Returns
    -------
    None
        This function does not return a value, it prints duplicate IDs if found.
    """
    # Open the file containing JSON data
    with open(apl_file, 'r') as file:
        data = file.read()

    # Parse the JSON
    parsed_data = json.loads(data)

    # Extract the activities
    activities = parsed_data['activities']

    # Create a set to store unique IDs
    unique_ids = set()

    # List to store duplicate IDs
    duplicate_ids = []

    # Iterate through activities
    for activity in activities:
        activity_id = activity['id']
        # Check if ID is already in the unique set
        if activity_id in unique_ids:
            duplicate_ids.append(activity_id)
        else:
            unique_ids.add(activity_id)

    # Print duplicate IDs, if any
    if duplicate_ids:
        print("Duplicate IDs found:")
        for duplicate_id in duplicate_ids:
            print(duplicate_id)
    else:
        print("No duplicate IDs found.")

    return