Metadata-Version: 2.1
Name: volkanoban
Version: 0.1.16
Summary: A stacking classifier with advanced explainability and visualization features
Author: Dr. Volkan OBAN
Author-email: volkanobn@gmail.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENCE.txt
Requires-Dist: numpy
Requires-Dist: pandas
Requires-Dist: scikit-learn
Requires-Dist: xgboost
Requires-Dist: lightgbm
Requires-Dist: catboost
Requires-Dist: lime
Requires-Dist: explainerdashboard
Requires-Dist: plotly
Requires-Dist: tabulate

# volkanoban Classifier

`volkanobanClassifier` is a stacking classifier that combines several machine learning models, including Random Forest, XGBoost, LightGBM, CatBoost, and more. The package also includes advanced features such as explainability through LIME, feature importance visualization, and a model dashboard using `explainerdashboard`.


## Features

- **Stacking Classifier:** Combines multiple machine learning models to improve performance.
- **Voting Classifier:** Uses soft voting for final predictions.
- **Explainability:** Provides insights using LIME (Local Interpretable Model-agnostic Explanations).
- **Feature Importance:** Visualizes the importance of features from multiple models.
- **Interactive Dashboard:** Creates an interactive dashboard using `explainerdashboard` for better model understanding.

## Installation

You can install the package using pip:

```bash
pip install volkanoban


## Usage Example 1: Breast Cancer Dataset

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from volkanoban import volkanobanClassifier

# Load the breast cancer dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Initialize the volkanobanClassifier
classifier = volkanobanClassifier()

# Train the classifier
X_train, X_test, y_train, y_test, y_pred = classifier.train(X, y)

# Evaluate performance
num_classes = len(np.unique(y_test))  # Number of unique classes in the test data
classifier.evaluate_performance(y_test, y_pred, num_classes)

# Perform LIME analysis
feature_names = X.columns  # Original feature names from the dataset
class_names = [str(i) for i in np.unique(y)]  # Class names from the target variable
classifier.lime_analysis(X_train, X_test, 0, feature_names, class_names)

# Plot feature importance
classifier.plot_feature_importance(X.columns)

# Run the ExplainerDashboard
classifier.run_explainer_dashboard(X_train, X_test, y_test, feature_names)


## Usage Example 2: Forest Cover Type Dataset

In this example, we use the fetch_covtype dataset from sklearn.datasets. The dataset includes several features about forest cover types, and the target variable represents different forest types.

import numpy as np
import pandas as pd
from sklearn.datasets import fetch_covtype
from volkanoban import volkanobanClassifier

# Load the forest cover type dataset
data = fetch_covtype()
X = pd.DataFrame(data.data)  # The feature matrix
y = pd.Series(data.target)  # The target variable

# Initialize the volkanobanClassifier
classifier = volkanobanClassifier()

# Train the classifier
X_train, X_test, y_train, y_test, y_pred = classifier.train(X, y)

# Evaluate performance
num_classes = len(np.unique(y_test))  # Number of unique classes in the test data
classifier.evaluate_performance(y_test, y_pred, num_classes)

# Perform LIME analysis
feature_names = X.columns  # Feature names from the dataset
class_names = [str(i) for i in np.unique(y)]  # Class names from the target variable
classifier.lime_analysis(X_train, X_test, 0, feature_names, class_names)

# Plot feature importance
classifier.plot_feature_importance(X.columns)

# Run the ExplainerDashboard
classifier.run_explainer_dashboard(X_train, X_test, y_test, feature_names)


## Usage Example 3: Multi-class Classification Example: Wine Dataset 

import numpy as np
import pandas as pd
from sklearn.datasets import load_wine
from volkanoban import volkanobanClassifier

# Load the wine dataset
data = load_wine()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Initialize the volkanobanClassifier
classifier = volkanobanClassifier()

# Train the classifier
X_train, X_test, y_train, y_test, y_pred = classifier.train(X, y)

# Evaluate performance for multi-class problem
num_classes = len(np.unique(y_test))  # Number of unique classes
classifier.evaluate_performance(y_test, y_pred, num_classes)

# Perform LIME analysis for multi-class
feature_names = X.columns  # Feature names from the dataset
class_names = [str(i) for i in np.unique(y)]  # Class names for the target variable
classifier.lime_analysis(X_train, X_test, 0, feature_names, class_names)

# Plot feature importance
classifier.plot_feature_importance(X.columns)

# Run the ExplainerDashboard for multi-class classification
classifier.run_explainer_dashboard(X_train, X_test, y_test, feature_names)



### Function Descriptions

#### evaluate_performance

This function evaluates the performance of the model using metrics like accuracy, precision, recall, F1 score, and confusion matrix. It prints the results in a well-formatted table.

Arguments:

y_true: Ground truth labels.

y_pred: Predicted labels by the model.

num_classes: Number of unique classes in the dataset.

Example Usage:

classifier.evaluate_performance(y_test, y_pred, num_classes)


This will output:

Accuracy: The overall accuracy of the model.
Confusion Matrix: A table displaying the true positives, false positives, false negatives, and true negatives for each class.
Performance Metrics: Precision, Recall, and F1 Score for each class.


#### lime_analysis

This function generates a LIME (Local Interpretable Model-agnostic Explanations) analysis for a specific instance in the test dataset. It explains how individual features contribute to the modelâ€™s prediction.

Arguments:

X_train: The scaled training dataset.
X_test: The scaled testing dataset.
index: The index of the instance in X_test to analyze.
feature_names: List of feature names from the dataset.
class_names: List of class names corresponding to the target variable.


Example Usage:

classifier.lime_analysis(X_train, X_test, 0, feature_names, class_names)

This will output:

A LIME explanation for the specified instance (test sample with index 0), showing how features influence the model's predictions.

#### plot_feature_importance

This function plots the feature importance for the stacking classifier. It calculates the importance of each feature from the individual models and visualizes it using a horizontal bar chart.

Arguments:

feature_names: List of feature names from the dataset.

Example Usage:

classifier.plot_feature_importance(feature_names)


This will display:

A bar plot showing the importance of each feature based on the contributions from the stacking model.

#### run_explainer_dashboard

This function launches an interactive dashboard using explainerdashboard, allowing you to explore the model's behavior, predictions, and explanations interactively. It includes visualizations for feature importance, SHAP values, and more.

Arguments:

X_train: The scaled training dataset.
X_test: The scaled testing dataset.
y_test: True labels for the testing dataset.
feature_names: List of feature names from the dataset.
dashboard_title: Title for the dashboard (optional).


Example Usage:

classifier.run_explainer_dashboard(X_train, X_test, y_test, feature_names, dashboard_title="Model Explainer")

This will:

Launch a local interactive dashboard on localhost:8051 where you can explore the modelâ€™s predictions and other details.

