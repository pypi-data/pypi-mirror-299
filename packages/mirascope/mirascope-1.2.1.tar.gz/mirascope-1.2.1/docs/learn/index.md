# Learn Mirascope

This section is designed to help you master Mirascope, a toolkit for building AI-powered applications with Large Language Models (LLMs).

Mirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs. Whether you're building chatbots, content generators, or complex AI-driven agent systems, Mirascope provides the tools you need to streamline your development process and create powerful, robust applications.

Our documentation is tailored for developers who have at least some experience with Python and LLMs. Whether you're coming from other development tool libraries or have worked directly with provider SDKs and APIs, Mirascope offers a familiar but enhanced experience.

## Key Features and Benefits

- **Type Safety and Superior Editor Support**: We've prioritized proper type hints and type safety, ensuring you get world-class editor support. This means fewer errors, better autocomplete, and a smoother development experience in as simple and seamless an interface as possible.
- **Provider-Agnostic Design**: Mirascope works seamlessly with multiple LLM providers, allowing you to switch between them effortlessly or use multiple providers in the same project.
- **Simplicity and Ease of Use**: We've designed Mirascope with simplicity in mind. You get powerful features without unnecessary complexity, making it easy to get started and scale your projects.
- **Comprehensive Tooling**: From prompt engineering to response parsing, Mirascope offers a complete suite of tools for every aspect of LLM application development.

## Core Components

Mirascope is built around the following core components, each designed to handle specific aspects of LLM interaction and application development. Here's a quick overview with links to detailed documentation:

1. [Prompts](./prompts.md): Learn how to create and manage prompts effectively.

2. [Calls](./calls.md): Understand how to make calls to LLMs using Mirascope.

3. [Streams](./streams.md): Explore streaming responses for real-time applications.

4. [Tools](./tools.md): Discover how to extend LLM capabilities with custom tools.

5. [Dynamic Configuration](./dynamic_configuration.md): Learn to adjust LLM behavior at runtime.

6. [Chaining](./chaining.md): Understand the art of chaining multiple LLM calls for complex tasks.

7. [JSON Mode](./json_mode.md): Work with structured data responses from LLMs.

8. [Response Models](./response_models.md): Define and use structured output models with automatic validation.

9. [Output Parsers](./output_parsers.md): Process and transform custom LLM output structures effectively.

10. [Async](./async.md): Maximize efficiecy with asynchronous programming.

11. [Evals](./evals.md): Apply core components to build evaluation strategies for your LLM applications.

12. [Agents](./agents.md): Put everything together to build advanced AI agents using Mirascope.

## Next Steps

We encourage you to dive into each component's documentation to gain a deeper understanding of Mirascope's capabilities. Start with the topics that align most closely with your immediate needs, but don't hesitate to explore all areas â€“ you might discover new ways to enhance your LLM applications!

As you progress, you'll find advanced topics and best practices throughout the documentation. These will help you optimize your use of Mirascope and build increasingly sophisticated AI-powered applications.

Happy learning, and welcome to the world of development with Mirascope!
